input {
  # here we'll define input from Filebeat, namely the host and port we're receiving beats from
  # remember that beats will actually be Nginx access log lines

    beats {
      # The port to listen on for filebeat connections.
      port => 5044
      # The IP address to listen for filebeat connections.
      host => "0.0.0.0"
    }
}

filter {
  # here we'll define rules for processing of received nginx access log lines
    if "nginx_access" in [tags] {
      grok {
        match => { "message" => ["%{IPORHOST:remote_ip} - %{DATA:user_name} \[%{HTTPDATE:time}\] \"%{WORD:http_method} %{DATA:url} HTTP/%{NUMBER:http_version}\" %{NUMBER:response_code} %{NUMBER:body_sent_bytes} \"%{DATA:referrer}\" \"%{DATA:agent}\""] }
        # remove_field => "message"
      }
      date {
        match => ["time", "dd/MMM/YYYY:HH:mm:ss Z"]
        target => "@timestamp"
        remove_field => "time"
      }
      useragent {
        source => "agent"
        target => "user_agent"
        remove_field => "agent"
      }
      #   geoip {
      #   source => "remote_ip"
      #   target => "geoip"
      #   database => "/usr/share/logstash/GeoLite2-City_20230519/GeoLite2-City.mmdb" # path to your DB location goes here
      #   fields => ["country_name", "country_code3", "region_name", "location"]
      #   add_field => [ "[geoip][coordinates]", "%{[geoip][longitude]}" ]
      #   add_field => [ "[geoip][coordinates]", "%{[geoip][latitude]}"  ]
      # }
    }

#  grok {
#   #  match => [ "message" , "%{COMBINEDAPACHELOG}+%{GREEDYDATA:extra_fields}"]
#   #  overwrite => [ "message" ]
#   match => { "message" => " %{IPORHOST:remote_ip} - %{DATA:user_name} \[%{HTTPDATE:access_time}\] \"%{WORD:http_method} %{DATA:url} HTTP/%{NUMBER:http_version}\" %{NUMBER:response_code} %{NUMBER:body_sent_bytes} \"%{DATA:referrer}\" \"%{DATA:agent}\"" }
#  }
#   geoip {
#         source => "remote_ip"
#         target => "geoip"
#         database => "/usr/share/logstash/GeoLite2-City_20230519/GeoLite2-City.mmdb" # path to your DB location goes here
#         fields => ["country_name", "country_code3", "region_name", "location"]
#         add_field => [ "[geoip][coordinates]", "%{[geoip][longitude]}" ]
#         add_field => [ "[geoip][coordinates]", "%{[geoip][latitude]}"  ]
#   }
#  mutate {
#    convert => ["response", "integer"]
#    convert => ["bytes", "integer"]
#    convert => ["responsetime", "float"]
#  }
#  geoip {
#    source => "clientip"
#    target => "geoip"
#    add_tag => [ "nginx-geoip" ]
#  }
  # if [type] == "nginx-access" {
  #   grok {
  #     match => { "message" => "%{NGINXACCESS}" }
  #   }
  #   geoip {
  #     source => "clientip"
  #   }
  # }
#  date {
#    match => [ "timestamp" , "dd/MMM/YYYY:HH:mm:ss Z" ]
#    remove_field => [ "timestamp" ]
#  }
  # date {
  #   match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
  #   }
  #   date {
  #       match => [ "timestamp" , "MMM dd, yyyy hh:mm:ss aa Z" ]
  # }
#  useragent {
#    source => "agent"
#  }
}

# First we want to check if the Logstash spits out our data. We tell it to show us the output on a standard output (our terminal)
output {
# here we'll define destination to which we desire to push processed and transformed log entries
 elasticsearch {
   hosts => ["http://elasticsearch:9200"]
   index => "nginx_logs"
   document_type => "nginx_logs"
   user     => "kibana_system"
   password => "changeme"
 }

   if "_grokparsefailure" in [tags] {
    # write events that didn't match to a file
    file { "path" => "/usr/share/logstash/grok_failures.txt" }
  } else {
  stdout { codec => rubydebug }
  }
}